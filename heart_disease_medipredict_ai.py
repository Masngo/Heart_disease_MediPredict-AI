# -*- coding: utf-8 -*-
"""Heart_disease_MediPredict AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1htrN9iHY4lbFRp9_iPf_4hKAmt-oTGjw
"""

### This project focus on:

### Download the dataset from Kaggle,

### Preprocess & split the data,

### Train 3 ML models (Logistic Regression, Decision Tree, Random Forest),

### Evaluate using accuracy, precision, recall, F1, ROC-AUC,

### Show Confusion Matrix & ROC curve,

### Summarize results in a neat DataFrame.
### Google Colab Template: Disease Prediction Toolkit (Heart Disease)
### Step 1: Setup Kaggle API (to download dataset)
### Go to Kaggle
### Create API Token (this downloads kaggle.json).

### Upload it into Colab:
from google.colab import files
files.upload()   # Upload kaggle.json

### Move it into place and install Kaggle CLI:
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Install kaggle
!pip install kaggle

### Download the Heart Disease dataset:
!kaggle datasets download -d redwankarimsony/heart-disease-data
!unzip heart-disease-data.zip

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report, roc_curve
)

### Load Dataset
df = pd.read_csv("heart_disease_uci.csv")
print("Dataset Shape:", df.shape)
display(df.head())

### EDA
print(df.info())
print(df.describe())

sns.countplot(x="num", data=df)
plt.title("Heart Disease Distribution")
plt.show()

# Correlation heatmap
plt.figure(figsize=(10,6))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap="coolwarm")
plt.show()

### Preprocessing
# Drop the 'ca' column due to a large number of missing values
df = df.drop("ca", axis=1)

# Impute missing values in numerical columns with the mean
for col in ["trestbps", "chol", "thalch", "oldpeak"]:
    df[col] = df[col].fillna(df[col].mean())

# Encode categorical features if needed
for col in df.select_dtypes(include=["object", "category"]):
    df[col] = LabelEncoder().fit_transform(df[col])

X = df.drop("num", axis=1)
y = df["num"]

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Feature Scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

### Model Training
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42)
}

results = {}

for name, model in models.items():
    model.fit(X_train, y_train)
    # Check if predict_proba exists before calling it
    y_proba = model.predict_proba(X_test) if hasattr(model, "predict_proba") else None

    y_pred = model.predict(X_test)

    results[name] = {
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred, average='weighted'),
        "Recall": recall_score(y_test, y_pred, average='weighted'),
        "F1 Score": f1_score(y_test, y_pred, average='weighted'),
        # Pass the full y_proba array for multiclass 'ovr' ROC AUC
        "ROC-AUC": roc_auc_score(y_test, y_proba, multi_class='ovr') if y_proba is not None else np.nan,
        "Confusion Matrix": confusion_matrix(y_test, y_pred)
    }

for name, res in results.items():
    print(f"\n--- {name} ---")
    for metric, val in res.items():
        if metric != "Confusion Matrix":
            print(f"{metric}: {val:.4f}")
    print("Confusion Matrix:\n", res["Confusion Matrix"])

### Visualization
# ROC Curve for Random Forest (Multiclass handled in a separate cell)
# The code below is commented out as roc_curve is for binary classification
# best_model = models["Random Forest"]
# y_proba = best_model.predict_proba(X_test)[:,1]

# fpr, tpr, _ = roc_curve(y_test, y_proba)
# roc_auc = roc_auc_score(y_test, y_proba)

# plt.figure(figsize=(6,4))
# plt.plot(fpr, tpr, label=f"AUC = {roc_auc:.2f}")
# plt.plot([0,1], [0,1], "k--")
# plt.xlabel("False Positive Rate")
# plt.ylabel("True Positive Rate")
# plt.title("ROC Curve - Random Forest")
# plt.legend()
# plt.show()

### Summary
import pandas as pd
summary = pd.DataFrame(results).T.drop("Confusion Matrix", axis=1)
summary

from google.colab import sheets
sheet = sheets.InteractiveSheet(df=summary)

from google.colab import sheets
sheet = sheets.InteractiveSheet(df=summary)

### Extended Google Colab Template with GridSearchCV
### Continue from the previous notebook after Previous cell (Summary).
### Hyperparameter Tuning - Decision Tree
from sklearn.model_selection import GridSearchCV

# Define parameter grid for Decision Tree
param_grid_dt = {
    "max_depth": [3, 5, 7, None],
    "min_samples_split": [2, 5, 10],
    "criterion": ["gini", "entropy"]
}

grid_dt = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid_dt,
    cv=5,
    scoring="accuracy",
    n_jobs=-1
)

grid_dt.fit(X_train, y_train)

print("Best Parameters for Decision Tree:", grid_dt.best_params_)
print("Best CV Score:", grid_dt.best_score_)

# Evaluate on test data
best_dt = grid_dt.best_estimator_
y_pred_dt = best_dt.predict(X_test)

print("\nOptimized Decision Tree Performance:")
print(classification_report(y_test, y_pred_dt))

### Hyperparameter Tuning - Random Forest
# Define parameter grid for Random Forest
param_grid_rf = {
    "n_estimators": [50, 100, 200],
    "max_depth": [5, 10, None],
    "min_samples_split": [2, 5, 10],
    "bootstrap": [True, False]
}

grid_rf = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid_rf,
    cv=5,
    scoring="accuracy",
    n_jobs=-1
)

grid_rf.fit(X_train, y_train)

print("Best Parameters for Random Forest:", grid_rf.best_params_)
print("Best CV Score:", grid_rf.best_score_)

# Evaluate on test data
best_rf = grid_rf.best_estimator_
y_pred_rf = best_rf.predict(X_test)

print("\nOptimized Random Forest Performance:")
print(classification_report(y_test, y_pred_rf))

### Feature Importance (for Random Forest)
### Get feature importance
importances = best_rf.feature_importances_
features = X.columns

feat_df = pd.DataFrame({"Feature": features, "Importance": importances})
feat_df = feat_df.sort_values("Importance", ascending=False)

plt.figure(figsize=(8,5))
sns.barplot(x="Importance", y="Feature", data=feat_df, palette="viridis")
plt.title("Feature Importance - Random Forest")
plt.show()

### Dataset Insights (Exploratory Data Analysis)
### Age Distribution
def plot_age_distribution(df):
    fig, ax = plt.subplots(figsize=(8, 5))
    sns.histplot(df['age'], bins=20, kde=True, color='teal', ax=ax)
    ax.set_title("Age Distribution of Patients")
    plt.show() # Display the plot directly in Colab

### Gender vs Disease
import seaborn as sns
import matplotlib.pyplot as plt
# import streamlit as st # Removed streamlit import

def plot_correlation_heatmap(df):
    fig, ax = plt.subplots(figsize=(10, 8))
    sns.heatmap(df.corr(numeric_only=True), annot=True, cmap="coolwarm", fmt=".2f", ax=ax)
    ax.set_title("Correlation Between Features", fontsize=14)
    plt.show()  # Display the plot directly in Colab


## Medical Feature Correlations
### Correlation Heatmap
import matplotlib.pyplot as plt
import seaborn as sns
# import streamlit as st # Removed streamlit import

def plot_gender_disease(df):
    # Create a figure and axis
    fig, ax = plt.subplots(figsize=(6, 4))

    # Create a count plot
    sns.countplot(data=df, x='sex', hue='num', palette="Set2", ax=ax) # Changed 'target' to 'num' based on the dataframe column name

    # Set x-tick labels (Assuming 0: Female, 1: Male based on previous cells)
    ax.set_xticklabels(["Female", "Male"])

    # Set the title of the plot
    ax.set_title("Heart Disease by Gender")

    # Display the plot in Colab
    plt.show()

!ls

import joblib

# Save the trained Random Forest model
joblib.dump(best_rf, "heart_disease_model.pkl")

print("✅ Model saved as heart_disease_model.pkl")

### Download the Model to Your Computer
from google.colab import files
files.download("heart_disease_model.pkl")

import joblib

# Save the trained Random Forest model
joblib.dump(best_rf, "heart_disease_model.pkl")

print("✅ Model saved as heart_disease_model.pkl")

### Final Results Summary
# Collect optimized model results
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score

opt_results = {
    "Optimized Decision Tree": accuracy_score(y_test, y_pred_dt),
    "Optimized Random Forest": accuracy_score(y_test, y_pred_rf)
}

print("Final Optimized Results:", opt_results)

### Load the Model Later
# Load saved model
loaded_model = joblib.load("heart_disease_model.pkl")

# Test prediction
sample = X_test[0].reshape(1, -1)   # first sample from test data
print("Prediction for sample:", loaded_model.predict(sample))